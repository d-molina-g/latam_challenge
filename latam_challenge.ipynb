{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60e8df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col, udf, date_format, rank, desc, count, row_number, explode\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python --version  3.8.13\n",
    "#spark --version 3.4.1 Using Scala version 2.12.17\n",
    "#emoji module --version 2.8.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c458f78",
   "metadata": {},
   "source": [
    "###      ENFOQUE OPTIMIZACIÓN DE TIEMPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcae4c4",
   "metadata": {},
   "source": [
    "#### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecc1be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "\n",
    "    # Creamos la sesión de Spark y cargamos el archivo\n",
    "    spark = SparkSession.builder.appName(\"TopDateUsers\").getOrCreate()\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    # Convertimos la columna date (para que tome solo lo necesario)\n",
    "    df = df.withColumn(\"date\", date_format(col(\"date\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "    # Unimos los tweets originales y los tweets en quotedTweet\n",
    "    df_combined = df.select(\"date\", \"user.username\").union(df.select(\"quotedTweet.date\", \"quotedTweet.user.username\").filter(col(\"quotedTweet.date\").isNotNull() & col(\"quotedTweet.user.username\").isNotNull()))\n",
    "\n",
    "    #df con las 10 fechas con más tweets\n",
    "    #esta parte se podría parametrizar, para este caso es valido ya que sabemos a priori el tamaño a mostrar en la lista,\n",
    "    #pero de ser un tamaño n no sería conveniente.\n",
    "    top_dates = df_combined.groupBy(\"date\").count().orderBy(desc(\"count\")).limit(10)\n",
    "\n",
    "    \n",
    "    #Obtenemos las 10 fechas con más tweets directamente como DataFrame\n",
    "    top_dates_df = top_dates.select(\"date\")\n",
    "\n",
    "    #Convertimos las fechas en una lista de objetos datetime.date\n",
    "    top_dates_list = [row.date for row in top_dates_df.collect()]\n",
    "\n",
    "    # Limpiams el DataFrame intermedio\n",
    "    top_dates_df.unpersist()\n",
    "    \n",
    "    # almacenamos en una lista las 10 fechas que más se repiten del df anterior\n",
    "    #top_dates_list = top_dates.select(\"date\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "    #creamos un df que solo contiene las fechas que están en la lista que generamos\n",
    "    filtered_df = df_combined.filter(col(\"date\").isin(top_dates_list))\n",
    "\n",
    "    #Limpiamos el DataFrame que agrupa los tweets (originales - retweets)\n",
    "    df_combined.unpersist()\n",
    "    \n",
    "    # Agrupamos el DataFrame por fecha y usuario y calcula el recuento de cada grupo\n",
    "    grouped_df = filtered_df.groupBy(\"date\", \"username\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "    # Usamos una ventana para obtener el usuario que más se repite por fecha mediante rank\n",
    "    window_spec = Window.partitionBy(\"date\").orderBy(desc(\"count\"))\n",
    "    top_users_df = grouped_df.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1)\n",
    "\n",
    "    #Seleccionar y ordenamos las columnas necesarias\n",
    "    result_df = top_users_df.select(\"date\", \"username\", \"count\").orderBy(desc(\"count\"))\n",
    "\n",
    "    # Recopilamos los resultados en una lista de tuplas\n",
    "    result = result_df.collect()\n",
    "\n",
    "    # Conviertimos el resultado en una lista de tuplas\n",
    "    result_list = [(row.date, row.username) for row in result]\n",
    "    \n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318f4e6",
   "metadata": {},
   "source": [
    "#### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f0f4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Creamos la sesión de Spark y cargamos el archivo\n",
    "    spark = SparkSession.builder.appName(\"TopEmojis\").getOrCreate()\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    # Función para dividir los emojis en caracteres individuales\n",
    "    def extract_individual_emojis(text):\n",
    "        return [c for c in text if c in emoji.EMOJI_DATA]\n",
    "\n",
    "    # UDF para dividir lo emojis y entregar un arreglo\n",
    "    extract_individual_emojis_udf = udf(extract_individual_emojis, ArrayType(StringType()))\n",
    "\n",
    "    # Agregamos una columna \"emojis\" al DataFrame con emojis individuales (en un arreglo)\n",
    "    df = df.withColumn(\"emojis\", extract_individual_emojis_udf(df[\"content\"]))\n",
    "    \n",
    "    # Desagrupamos los arreglos del campo emojis en filas separadas (emoji)\n",
    "    df = df.select(\"emojis\").withColumn(\"emoji\", explode(\"emojis\")).filter(col(\"emoji\") != \"\")\n",
    "\n",
    "    # Contamos la frecuencia de cada emoji en todo el DataFrame\n",
    "    emoji_counts = df.groupBy(\"emoji\").count()\n",
    "\n",
    "    # Ordenamos los resultados por conteo en orden descendente\n",
    "    emoji_counts = emoji_counts.orderBy(desc(\"count\"))\n",
    "\n",
    "    # Obtenemos los 10 emojis con mayor conteo\n",
    "    top_10_emojis = emoji_counts.limit(10).collect()\n",
    "\n",
    "    return top_10_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c947ab8c",
   "metadata": {},
   "source": [
    "#### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c29716d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Creamos la sesión de Spark y cargamos el archivo JSON en un DataFrame\n",
    "    spark = SparkSession.builder.appName(\"TopMentionedUsers\").getOrCreate()\n",
    "    df_mention = spark.read.json(file_path)\n",
    "\n",
    "    #Convertimos la columna date en\n",
    "    df_mention = df_mention.withColumn(\"date\", date_format(col(\"date\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "    #Unimos los tweets originales y los tweets en quotedTweet\n",
    "    df_combined_mention = df_mention.select(\"mentionedusers.username\").filter(col(\"mentionedUsers.username\").isNotNull()).union(df_mention.select(\"quotedTweet.mentionedUsers.username\").filter(col(\"quotedTweet.mentionedUsers.username\").isNotNull()))\n",
    "\n",
    "    #Descomponemos la columna \"usernames\" en filas individuales\n",
    "    df = df_combined_mention.selectExpr(\"explode(username) as usernames\")\n",
    "\n",
    "    # Contamos el número de veces que aparece cada usuario\n",
    "    user_counts = df.groupBy(\"usernames\").count()\n",
    "\n",
    "    #Ordenamos en orden descendente por conteo y seleccionar los 10 primeros usuarios\n",
    "    top_10_users = user_counts.orderBy(col(\"count\").desc()).limit(10)\n",
    "\n",
    "    # Recopilamos los resultados en una lista de tuplas (usuario, conteo)\n",
    "    result_list = [(row.usernames, row[\"count\"]) for row in top_10_users.collect()]\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f8607759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 15:59:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "23/09/26 15:59:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 ms, sys: 0 ns, total: 20.2 ms\n",
      "Wall time: 3.12 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 15:59:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 ms, sys: 264 µs, total: 12 ms\n",
      "Wall time: 1.77 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 556:============================>                           (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.7 ms, sys: 5.35 ms, total: 12 ms\n",
      "Wall time: 1.85 s\n",
      "\n",
      "\n",
      "[('2021-02-19', 'Preetm91'), ('2021-02-18', 'neetuanjle_nitu'), ('2021-02-13', 'MaanDee08215437'), ('2021-02-17', 'RaaJVinderkaur'), ('2021-02-16', 'jot__b'), ('2021-02-23', 'preetysaini321'), ('2021-02-15', 'jot__b'), ('2021-02-14', 'Gurpreetd86'), ('2021-02-20', 'MangalJ23056160'), ('2021-02-12', 'rebelpacifist')]\n",
      "\n",
      "\n",
      "[Row(emoji='🙏', count=7286), Row(emoji='😂', count=3072), Row(emoji='🚜', count=2972), Row(emoji='✊', count=2411), Row(emoji='🌾', count=2363), Row(emoji='🏻', count=2080), Row(emoji='❤', count=1779), Row(emoji='🤣', count=1668), Row(emoji='🏽', count=1218), Row(emoji='👇', count=1108)]\n",
      "\n",
      "\n",
      "[('narendramodi', 2623), ('Kisanektamorcha', 2045), ('RakeshTikaitBKU', 1848), ('PMOIndia', 1560), ('GretaThunberg', 1274), ('RahulGandhi', 1252), ('rihanna', 1142), ('DelhiPolice', 1134), ('RaviSinghKA', 1127), ('UNHumanRights', 1057)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Modificar file_path:\n",
    "file_path = \"/home/david/Escritorio/farmers-protest-tweets-2021-2-4.json\"\n",
    "%time top_tweets_time = q1_time(file_path)\n",
    "%time top_emojis_time = q2_time(file_path)\n",
    "%time top_mentions_time = q3_time(file_path)\n",
    "\n",
    "print('\\n')\n",
    "print(top_tweets_time)\n",
    "print('\\n')\n",
    "print(top_emojis_time)\n",
    "print('\\n')\n",
    "print(top_mentions_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b20177",
   "metadata": {},
   "source": [
    "###      ENFOQUE OPTIMIZACIÓN DE MEMORIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c8616",
   "metadata": {},
   "source": [
    "#### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "593e0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    #Creamos la sesión de Spark y cargamos el archivo\n",
    "    spark = SparkSession.builder.appName(\"TopDateUsers\").getOrCreate()\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    #Convertimos la columna de fecha (para que tome solo lo necesario)\n",
    "    df = df.withColumn(\"date\", date_format(col(\"date\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "    # Unimos los tweets originales y los tweets en quotedTweet\n",
    "    df_combined = df.select(\"date\", \"user.username\").union(df.select(\"quotedTweet.date\", \"quotedTweet.user.username\").filter(col(\"quotedTweet.date\").isNotNull() & col(\"quotedTweet.user.username\").isNotNull()))\n",
    "\n",
    "    #df con las 10 fechas con más tweets\n",
    "    #esta parte se podría parametrizar, para este caso es válido ya que sabemos a priori el tamaño a mostrar en la lista,\n",
    "    #pero de ser un tamaño n no sería conveniente.\n",
    "    \n",
    "    #Para liberar memoria realizo primero la agregación (para contar la cantidad por fecha) y luego obtengo el top ordenandolos\n",
    "    #En dos pasos separados\n",
    "    agg_df = df_combined.groupBy(\"date\").agg(count(\"*\").alias(\"count\"))\n",
    "    \n",
    "    # Ordenar los resultados\n",
    "    top_dates = agg_df.orderBy(desc(\"count\")).limit(10)\n",
    "        \n",
    "    #Obtenemos las 10 fechas con más tweets directamente como DataFrame\n",
    "    top_dates_df = top_dates.select(\"date\")\n",
    "\n",
    "    # Obtenemos las 10 fechas como lista\n",
    "    top_dates_list = [row.date for row in top_dates.collect()]\n",
    "\n",
    "    # Filtrar el DataFrame original\n",
    "    filtered_df = df_combined.filter(col(\"date\").isin(top_dates_list))\n",
    "\n",
    "    #Agrupamos el DataFrame por fecha y usuario y calcula el recuento de cada grupo\n",
    "    grouped_df = filtered_df.groupBy(\"date\", \"username\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "    # Usamos una ventana para obtener el usuario que más se repite por fecha mediante rank\n",
    "    window_spec = Window.partitionBy(\"date\").orderBy(desc(\"count\"))\n",
    "    top_users_df = grouped_df.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1)\n",
    "\n",
    "    # Seleccionamos las columnas necesarias\n",
    "    result_df = top_users_df.select(\"date\", \"username\", \"count\")\n",
    "    \n",
    "    #Ordenamos el DataFrame por count en orden descendente\n",
    "    result_df = result_df.orderBy(desc(\"count\"))\n",
    "\n",
    "    #Recopilamos los resultados en una lista de tuplas\n",
    "    result = result_df.collect()\n",
    "\n",
    "    #Conviertimos el resultado en una lista de tuplas\n",
    "    result_list = [(row.date, row.username) for row in result]\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7172b3c6",
   "metadata": {},
   "source": [
    "#### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfa22501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Creamos la sesión de Spark y cargar el archivo\n",
    "    spark = SparkSession.builder.appName(\"TopEmojis\").getOrCreate()\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    # Función para dividir los emojis en caracteres individuales\n",
    "    def extract_individual_emojis(text):\n",
    "        return [c for c in text if c in emoji.EMOJI_DATA]\n",
    "\n",
    "    # UDF para dividir los emojis y entregar un arreglo\n",
    "    extract_individual_emojis_udf = udf(extract_individual_emojis, ArrayType(StringType()))\n",
    "\n",
    "    # Agregamos una columna \"emojis\" al DataFrame con emojis individuales (en un arreglo)\n",
    "    df = df.withColumn(\"emojis\", extract_individual_emojis_udf(df[\"content\"]))\n",
    "    \n",
    "    # Desagrupamos los arreglos del campo emojis en filas separadas (emoji)\n",
    "    df = df.select(\"emojis\").withColumn(\"emoji\", explode(\"emojis\")).filter(col(\"emoji\") != \"\")\n",
    "\n",
    "    # Contamos la frecuencia de cada emoji en todo el DataFrame\n",
    "    emoji_counts = df.groupBy(\"emoji\").count()\n",
    "\n",
    "    #Obtenemos los 10 emojis con mayor conteo (usando limit antes de ordenar)\n",
    "    top_10_emojis = emoji_counts.orderBy(desc(\"count\")).limit(10).collect()\n",
    "\n",
    "    # Liberamos la memoria eliminando el DataFrame df\n",
    "    df.unpersist()\n",
    "\n",
    "    return top_10_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd862bc",
   "metadata": {},
   "source": [
    "#### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bab79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Creamos una sesión de Spark\n",
    "    spark = SparkSession.builder.appName(\"TopMentionedUsers\").getOrCreate()\n",
    "\n",
    "    # Cargamos el archivo JSON en un DataFrame\n",
    "    df_mention = spark.read.json(file_path)\n",
    "\n",
    "    # Convertimos la columna date en el formato necesario\n",
    "    df_mention = df_mention.withColumn(\"date\", date_format(col(\"date\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "    # Unir los tweets originales y los tweets en quotedTweet\n",
    "    df_combined_mention = df_mention.select(explode(\"mentionedUsers.username\").alias(\"username\"))\n",
    "\n",
    "    # Contar el número de veces que aparece cada usuario\n",
    "    user_counts = df_combined_mention.groupBy(\"username\").count()\n",
    "\n",
    "    # Ordenar en orden descendente por conteo y seleccionar los 10 primeros usuarios\n",
    "    top_10_users = user_counts.orderBy(desc(\"count\")).limit(10)\n",
    "\n",
    "    # Recopilar los resultados en una lista de tuplas (usuario, conteo)\n",
    "    result_list = [(row.username, row[\"count\"]) for row in top_10_users.collect()]\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "79e2b6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 15:58:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "23/09/26 15:58:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 ms, sys: 8.86 ms, total: 21.4 ms\n",
      "Wall time: 3.26 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 15:58:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 ms, sys: 1.12 ms, total: 12.8 ms\n",
      "Wall time: 1.94 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 530:>                                                        (0 + 8) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.52 ms, sys: 0 ns, total: 7.52 ms\n",
      "Wall time: 1.41 s\n",
      "\n",
      "\n",
      "[('2021-02-19', 'Preetm91'), ('2021-02-18', 'neetuanjle_nitu'), ('2021-02-13', 'MaanDee08215437'), ('2021-02-17', 'RaaJVinderkaur'), ('2021-02-16', 'jot__b'), ('2021-02-23', 'preetysaini321'), ('2021-02-15', 'jot__b'), ('2021-02-14', 'Gurpreetd86'), ('2021-02-20', 'MangalJ23056160'), ('2021-02-12', 'rebelpacifist')]\n",
      "\n",
      "\n",
      "[Row(emoji='🙏', count=7286), Row(emoji='😂', count=3072), Row(emoji='🚜', count=2972), Row(emoji='✊', count=2411), Row(emoji='🌾', count=2363), Row(emoji='🏻', count=2080), Row(emoji='❤', count=1779), Row(emoji='🤣', count=1668), Row(emoji='🏽', count=1218), Row(emoji='👇', count=1108)]\n",
      "\n",
      "\n",
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "# Modificar file_path:\n",
    "file_path = \"/home/david/Escritorio/farmers-protest-tweets-2021-2-4.json\"\n",
    "%time top_tweets_memory = q1_memory(file_path)\n",
    "%time top_mentions_memory = q2_memory(file_path)\n",
    "%time top_emojis_memory = q3_memory(file_path)\n",
    "\n",
    "print('\\n')\n",
    "print(top_tweets_memory)\n",
    "print('\\n')\n",
    "print(top_mentions_memory)\n",
    "print('\\n')\n",
    "print(top_emojis_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b9556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
