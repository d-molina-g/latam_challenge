{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e8df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, date_format, rank, when, desc, count, mean, row_number, regexp_extract, explode, split, regexp_replace\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "import emoji\n",
    "import re\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecc1be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "\n",
    "    # Creamos la sesiÃ³n de Spark y cargamos el archivo\n",
    "    spark = SparkSession.builder.appName(\"TopDateUsers\").getOrCreate()\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    # Convertimos la columna date (para que tome solo lo necesario)\n",
    "    df = df.withColumn(\"date\", date_format(col(\"date\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "    # Unimos los tweets originales y los tweets en quotedTweet\n",
    "    df_combined = df.select(\"date\", \"user.username\").union(df.select(\"quotedTweet.date\", \"quotedTweet.user.username\").filter(col(\"quotedTweet.date\").isNotNull() & col(\"quotedTweet.user.username\").isNotNull()))\n",
    "\n",
    "    #df con las 10 fechas con mÃ¡s tweets\n",
    "    #esta parte se podrÃ­a parametrizar, para este caso es valido ya que sabemos a priori el tamaÃ±o a mostrar en la lista,\n",
    "    #pero de ser un tamaÃ±o n no serÃ­a conveniente.\n",
    "    top_dates = df_combined.groupBy(\"date\").count().orderBy(desc(\"count\")).limit(10)\n",
    "\n",
    "    \n",
    "    #Obtenemos las 10 fechas con mÃ¡s tweets directamente como DataFrame\n",
    "    top_dates_df = top_dates.select(\"date\")\n",
    "\n",
    "    #Convertimos las fechas en una lista de objetos datetime.date\n",
    "    top_dates_list = [row.date for row in top_dates_df.collect()]\n",
    "\n",
    "    # Limpiams el DataFrame intermedio\n",
    "    top_dates_df.unpersist()\n",
    "    \n",
    "    # almacenamos en una lista las 10 fechas que mÃ¡s se repiten del df anterior\n",
    "    #top_dates_list = top_dates.select(\"date\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "    #creamos un df que solo contiene las fechas que estÃ¡n en la lista que generamos\n",
    "    filtered_df = df_combined.filter(col(\"date\").isin(top_dates_list))\n",
    "\n",
    "    #Limpiamos el DataFrame que agrupa los tweets (originales - retweets)\n",
    "    df_combined.unpersist()\n",
    "    \n",
    "    # Agrupamos el DataFrame por fecha y usuario y calcula el recuento de cada grupo\n",
    "    grouped_df = filtered_df.groupBy(\"date\", \"username\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "    # Usamos una ventana para obtener el usuario que mÃ¡s se repite por fecha mediante rank\n",
    "    window_spec = Window.partitionBy(\"date\").orderBy(desc(\"count\"))\n",
    "    top_users_df = grouped_df.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1)\n",
    "\n",
    "    #Seleccionar y ordenamos las columnas necesarias\n",
    "    result_df = top_users_df.select(\"date\", \"username\", \"count\").orderBy(desc(\"count\"))\n",
    "\n",
    "    # Recopilamos los resultados en una lista de tuplas\n",
    "    result = result_df.collect()\n",
    "\n",
    "    # Conviertimos el resultado en una lista de tuplas\n",
    "    result_list = [(row.date, row.username) for row in result]\n",
    "    \n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f0f4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Creamos la sesiÃ³n de Spark y cargamos el archivo\n",
    "    spark = SparkSession.builder.appName(\"TopEmojis\").getOrCreate()\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    # FunciÃ³n para dividir los emojis en caracteres individuales\n",
    "    def extract_individual_emojis(text):\n",
    "        return [c for c in text if c in emoji.EMOJI_DATA]\n",
    "\n",
    "    # UDF para dividir lo emojis y entregar un arreglo\n",
    "    extract_individual_emojis_udf = udf(extract_individual_emojis, ArrayType(StringType()))\n",
    "\n",
    "    # Agregamos una columna \"emojis\" al DataFrame con emojis individuales (en un arreglo)\n",
    "    df = df.withColumn(\"emojis\", extract_individual_emojis_udf(df[\"content\"]))\n",
    "    \n",
    "    # Desagrupamos los arreglos del campo emojis en filas separadas (emoji)\n",
    "    df = df.select(\"emojis\").withColumn(\"emoji\", explode(\"emojis\")).filter(col(\"emoji\") != \"\")\n",
    "\n",
    "    # Contamos la frecuencia de cada emoji en todo el DataFrame\n",
    "    emoji_counts = df.groupBy(\"emoji\").count()\n",
    "\n",
    "    # Ordenamos los resultados por conteo en orden descendente\n",
    "    emoji_counts = emoji_counts.orderBy(desc(\"count\"))\n",
    "\n",
    "    # Obtenemos los 10 emojis con mayor conteo\n",
    "    top_10_emojis = emoji_counts.limit(10).collect()\n",
    "\n",
    "    return top_10_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c29716d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Crear una sesiÃ³n de Spark\n",
    "    spark = SparkSession.builder.appName(\"TopMentionedUsers\").getOrCreate()\n",
    "    # Cargar el archivo JSON en un DataFrame\n",
    "    df_mention = spark.read.json(file_path)\n",
    "\n",
    "    # Convierto la columna date en\n",
    "    df_mention = df_mention.withColumn(\"date\", date_format(col(\"date\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "    # Unir los tweets originales y los tweets en quotedTweet\n",
    "    df_combined_mention = df_mention.select(\"mentionedusers.username\").filter(col(\"mentionedUsers.username\").isNotNull()).union(df_mention.select(\"quotedTweet.mentionedUsers.username\").filter(col(\"quotedTweet.mentionedUsers.username\").isNotNull()))\n",
    "\n",
    "    # Descomponer la columna \"usernames\" en filas individuales\n",
    "    df = df_combined_mention.selectExpr(\"explode(username) as usernames\")\n",
    "\n",
    "    # Contar el nÃºmero de veces que aparece cada usuario\n",
    "    user_counts = df.groupBy(\"usernames\").count()\n",
    "\n",
    "    # Ordenar en orden descendente por conteo y seleccionar los 10 primeros usuarios\n",
    "    top_10_users = user_counts.orderBy(col(\"count\").desc()).limit(10)\n",
    "\n",
    "    # Recopilar los resultados en una lista de tuplas (usuario, conteo)\n",
    "    result_list = [(row.usernames, row[\"count\"]) for row in top_10_users.collect()]\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8607759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 14:58:29 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.2 ms, sys: 7.76 ms, total: 22 ms\n",
      "Wall time: 3.26 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 14:58:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.79 ms, sys: 11.1 ms, total: 13.9 ms\n",
      "Wall time: 1.92 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 290:============================>                           (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 ms, sys: 483 Âµs, total: 12.4 ms\n",
      "Wall time: 2.01 s\n",
      "\n",
      "\n",
      "[('2021-02-19', 'Preetm91'), ('2021-02-18', 'neetuanjle_nitu'), ('2021-02-13', 'MaanDee08215437'), ('2021-02-17', 'RaaJVinderkaur'), ('2021-02-16', 'jot__b'), ('2021-02-23', 'preetysaini321'), ('2021-02-15', 'jot__b'), ('2021-02-14', 'Gurpreetd86'), ('2021-02-20', 'MangalJ23056160'), ('2021-02-12', 'rebelpacifist')]\n",
      "\n",
      "\n",
      "[('narendramodi', 2623), ('Kisanektamorcha', 2045), ('RakeshTikaitBKU', 1848), ('PMOIndia', 1560), ('GretaThunberg', 1274), ('RahulGandhi', 1252), ('rihanna', 1142), ('DelhiPolice', 1134), ('RaviSinghKA', 1127), ('UNHumanRights', 1057)]\n",
      "\n",
      "\n",
      "[Row(emoji='ðŸ™', count=7286), Row(emoji='ðŸ˜‚', count=3072), Row(emoji='ðŸšœ', count=2972), Row(emoji='âœŠ', count=2411), Row(emoji='ðŸŒ¾', count=2363), Row(emoji='ðŸ»', count=2080), Row(emoji='â¤', count=1779), Row(emoji='ðŸ¤£', count=1668), Row(emoji='ðŸ½', count=1218), Row(emoji='ðŸ‘‡', count=1108)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Modificar file_path:\n",
    "file_path = \"/home/david/Escritorio/farmers-protest-tweets-2021-2-4.json\"\n",
    "%time top_tweets_time = q1_time(file_path)\n",
    "%time top_emojis_time = q2_time(file_path)\n",
    "%time top_mentions_time = q3_time(file_path)\n",
    "\n",
    "print('\\n')\n",
    "print(top_tweets_time)\n",
    "print('\\n')\n",
    "print(top_mentions_time)\n",
    "print('\\n')\n",
    "print(top_emojis_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b20177",
   "metadata": {},
   "source": [
    "### Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "593e0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    #Creamos la sesiÃ³n de Spark y cargamos el archivo\n",
    "    spark = SparkSession.builder.appName(\"TopDateUsers\").getOrCreate()\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    #Convertimos la columna de fecha (para que tome solo lo necesario)\n",
    "    df = df.withColumn(\"date\", date_format(col(\"date\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "    # Unimos los tweets originales y los tweets en quotedTweet\n",
    "    df_combined = df.select(\"date\", \"user.username\").union(df.select(\"quotedTweet.date\", \"quotedTweet.user.username\").filter(col(\"quotedTweet.date\").isNotNull() & col(\"quotedTweet.user.username\").isNotNull()))\n",
    "\n",
    "    #df con las 10 fechas con mÃ¡s tweets\n",
    "    #esta parte se podrÃ­a parametrizar, para este caso es vÃ¡lido ya que sabemos a priori el tamaÃ±o a mostrar en la lista,\n",
    "    #pero de ser un tamaÃ±o n no serÃ­a conveniente.\n",
    "    \n",
    "    #Para liberar memoria realizo primero la agregaciÃ³n (para contar la cantidad por fecha) y luego obtengo el top ordenandolos\n",
    "    #En dos pasos separados\n",
    "    agg_df = df_combined.groupBy(\"date\").agg(count(\"*\").alias(\"count\"))\n",
    "    \n",
    "    # Ordenar los resultados\n",
    "    top_dates = agg_df.orderBy(desc(\"count\")).limit(10)\n",
    "        \n",
    "    #Obtenemos las 10 fechas con mÃ¡s tweets directamente como DataFrame\n",
    "    top_dates_df = top_dates.select(\"date\")\n",
    "\n",
    "    # Obtenemos las 10 fechas como lista\n",
    "    top_dates_list = [row.date for row in top_dates.collect()]\n",
    "\n",
    "    # Filtrar el DataFrame original\n",
    "    filtered_df = df_combined.filter(col(\"date\").isin(top_dates_list))\n",
    "\n",
    "    #Agrupamos el DataFrame por fecha y usuario y calcula el recuento de cada grupo\n",
    "    grouped_df = filtered_df.groupBy(\"date\", \"username\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "    # Usamos una ventana para obtener el usuario que mÃ¡s se repite por fecha mediante rank\n",
    "    window_spec = Window.partitionBy(\"date\").orderBy(desc(\"count\"))\n",
    "    top_users_df = grouped_df.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1)\n",
    "\n",
    "    # Seleccionamos las columnas necesarias\n",
    "    result_df = top_users_df.select(\"date\", \"username\", \"count\")\n",
    "    \n",
    "    #Ordenamos el DataFrame por count en orden descendente\n",
    "    result_df = result_df.orderBy(desc(\"count\"))\n",
    "\n",
    "    #Recopilamos los resultados en una lista de tuplas\n",
    "    result = result_df.collect()\n",
    "\n",
    "    #Conviertimos el resultado en una lista de tuplas\n",
    "    result_list = [(row.date, row.username) for row in result]\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35a0df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Creamos la sesiÃ³n de Spark y cargar el archivo\n",
    "    spark = SparkSession.builder.appName(\"TopEmojis\").getOrCreate()\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    # FunciÃ³n para dividir los emojis en caracteres individuales\n",
    "    def extract_individual_emojis(text):\n",
    "        return [c for c in text if c in emoji.EMOJI_DATA]\n",
    "\n",
    "    # UDF para dividir los emojis y entregar un arreglo\n",
    "    extract_individual_emojis_udf = udf(extract_individual_emojis, ArrayType(StringType()))\n",
    "\n",
    "    # Agregamos una columna \"emojis\" al DataFrame con emojis individuales (en un arreglo)\n",
    "    df = df.withColumn(\"emojis\", extract_individual_emojis_udf(df[\"content\"]))\n",
    "    \n",
    "    # Desagrupamos los arreglos del campo emojis en filas separadas (emoji)\n",
    "    df = df.select(\"emojis\").withColumn(\"emoji\", explode(\"emojis\")).filter(col(\"emoji\") != \"\")\n",
    "\n",
    "    # Contamos la frecuencia de cada emoji en todo el DataFrame\n",
    "    emoji_counts = df.groupBy(\"emoji\").count()\n",
    "\n",
    "    #Obtenemos los 10 emojis con mayor conteo (usando limit antes de ordenar)\n",
    "    top_10_emojis = emoji_counts.orderBy(desc(\"count\")).limit(10).collect()\n",
    "\n",
    "    # Liberamos la memoria eliminando el DataFrame df\n",
    "    df.unpersist()\n",
    "\n",
    "    return top_10_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42523e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79e2b6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 15:12:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 ms, sys: 1.61 ms, total: 29.1 ms\n",
      "Wall time: 3.27 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 15:12:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 366:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 ms, sys: 2.01 ms, total: 12 ms\n",
      "Wall time: 2.02 s\n",
      "\n",
      "\n",
      "[('2021-02-19', 'Preetm91'), ('2021-02-18', 'neetuanjle_nitu'), ('2021-02-13', 'MaanDee08215437'), ('2021-02-17', 'RaaJVinderkaur'), ('2021-02-16', 'jot__b'), ('2021-02-23', 'preetysaini321'), ('2021-02-15', 'jot__b'), ('2021-02-14', 'Gurpreetd86'), ('2021-02-20', 'MangalJ23056160'), ('2021-02-12', 'rebelpacifist')]\n",
      "\n",
      "\n",
      "[Row(emoji='ðŸ™', count=7286), Row(emoji='ðŸ˜‚', count=3072), Row(emoji='ðŸšœ', count=2972), Row(emoji='âœŠ', count=2411), Row(emoji='ðŸŒ¾', count=2363), Row(emoji='ðŸ»', count=2080), Row(emoji='â¤', count=1779), Row(emoji='ðŸ¤£', count=1668), Row(emoji='ðŸ½', count=1218), Row(emoji='ðŸ‘‡', count=1108)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 366:===================================>                     (5 + 3) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso:\n",
    "file_path = \"/home/david/Escritorio/farmers-protest-tweets-2021-2-4.json\"\n",
    "%time top_tweets_memory = q1_memory(file_path)\n",
    "%time top_mentions_memory = q2_memory(file_path)\n",
    "\n",
    "print('\\n')\n",
    "print(top_tweets_memory)\n",
    "print('\\n')\n",
    "print(top_mentions_memory)\n",
    "print('\\n')\n",
    "#print(top_emojis_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b9556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
