{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60e8df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col, udf, date_format, rank, desc, count, row_number, explode\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e942b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#David Molina G.\n",
    "#python --version  3.8.13\n",
    "#spark --version 3.4.1 Using Scala version 2.12.17\n",
    "#emoji module --version 2.8.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c942082",
   "metadata": {},
   "source": [
    "###      ENFOQUE OPTIMIZACIÓN DE TIEMPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee00d099",
   "metadata": {},
   "source": [
    "#### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ecc1be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "\n",
    "    # Creamos la sesión de Spark y cargamos el archivo\n",
    "    spark = SparkSession.builder.appName(\"TopDateUsers\").getOrCreate()\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    # Convertimos la columna date (para que tome solo lo necesario)\n",
    "    df = df.withColumn(\"date\", date_format(col(\"date\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "    # Unimos los tweets originales y los tweets en quotedTweet\n",
    "    df_combined = df.select(\"date\", \"user.username\").union(df.select(\"quotedTweet.date\", \"quotedTweet.user.username\").filter(col(\"quotedTweet.date\").isNotNull() & col(\"quotedTweet.user.username\").isNotNull()))\n",
    "\n",
    "    #df con las 10 fechas con más tweets\n",
    "    #esta parte se podría parametrizar, para este caso es valido ya que sabemos a priori el tamaño a mostrar en la lista,\n",
    "    #pero de ser un tamaño n no sería conveniente.\n",
    "    top_dates = df_combined.groupBy(\"date\").count().orderBy(desc(\"count\")).limit(10)\n",
    "\n",
    "    \n",
    "    #Obtenemos las 10 fechas con más tweets directamente como DataFrame\n",
    "    top_dates_df = top_dates.select(\"date\")\n",
    "\n",
    "    #Convertimos las fechas en una lista de objetos datetime.date\n",
    "    top_dates_list = [row.date for row in top_dates_df.collect()]\n",
    "\n",
    "    # Limpiams el DataFrame intermedio\n",
    "    top_dates_df.unpersist()\n",
    "    \n",
    "\n",
    "    #creamos un df que solo contiene las fechas que están en la lista que generamos\n",
    "    filtered_df = df_combined.filter(col(\"date\").isin(top_dates_list))\n",
    "\n",
    "    #Limpiamos el DataFrame que agrupa los tweets (originales - retweets)\n",
    "    df_combined.unpersist()\n",
    "    \n",
    "    # Agrupamos el DataFrame por fecha y usuario y calcula el recuento de cada grupo\n",
    "    grouped_df = filtered_df.groupBy(\"date\", \"username\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "    # Usamos una ventana para obtener el usuario que más se repite por fecha mediante rank\n",
    "    window_spec = Window.partitionBy(\"date\").orderBy(desc(\"count\"))\n",
    "    top_users_df = grouped_df.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1)\n",
    "\n",
    "    #Seleccionar y ordenamos las columnas necesarias\n",
    "    result_df = top_users_df.select(\"date\", \"username\", \"count\").orderBy(desc(\"count\"))\n",
    "\n",
    "    # Recopilamos los resultados en una lista de tuplas\n",
    "    result = result_df.collect()\n",
    "\n",
    "    # Conviertimos el resultado en una lista de tuplas\n",
    "    result_list = [(row.date, row.username) for row in result]\n",
    "    \n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33a1f2",
   "metadata": {},
   "source": [
    "#### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f0f4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Creamos la sesión de Spark y cargamos el archivo\n",
    "    spark = SparkSession.builder.appName(\"TopEmojis\").getOrCreate()\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    # Función para dividir los emojis en caracteres individuales\n",
    "    def extract_individual_emojis(text):\n",
    "        return [c for c in text if c in emoji.EMOJI_DATA]\n",
    "\n",
    "    # UDF para dividir lo emojis y entregar un arreglo\n",
    "    extract_individual_emojis_udf = udf(extract_individual_emojis, ArrayType(StringType()))\n",
    "\n",
    "    # Agregamos una columna \"emojis\" al DataFrame con emojis individuales (en un arreglo)\n",
    "    df = df.withColumn(\"emojis\", extract_individual_emojis_udf(df[\"content\"]))\n",
    "    \n",
    "    # Desagrupamos los arreglos del campo emojis en filas separadas (emoji)\n",
    "    df = df.select(\"emojis\").withColumn(\"emoji\", explode(\"emojis\")).filter(col(\"emoji\") != \"\")\n",
    "\n",
    "    # Contamos la frecuencia de cada emoji en todo el DataFrame\n",
    "    emoji_counts = df.groupBy(\"emoji\").count()\n",
    "\n",
    "    # Ordenamos los resultados por conteo en orden descendente\n",
    "    emoji_counts = emoji_counts.orderBy(desc(\"count\"))\n",
    "\n",
    "    # Obtenemos los 10 emojis con mayor conteo\n",
    "    top_10_emojis = emoji_counts.limit(10).collect()\n",
    "\n",
    "    return top_10_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54467b",
   "metadata": {},
   "source": [
    "#### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c29716d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Creamos la sesión de Spark y cargamos el archivo JSON en un DataFrame\n",
    "    spark = SparkSession.builder.appName(\"TopMentionedUsers\").getOrCreate()\n",
    "    df_mention = spark.read.json(file_path)\n",
    "\n",
    "    #Convertimos la columna date en\n",
    "    df_mention = df_mention.withColumn(\"date\", date_format(col(\"date\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "    #Unimos los tweets originales y los tweets en quotedTweet\n",
    "    df_combined_mention = df_mention.select(\"mentionedusers.username\").filter(col(\"mentionedUsers.username\").isNotNull()).union(df_mention.select(\"quotedTweet.mentionedUsers.username\").filter(col(\"quotedTweet.mentionedUsers.username\").isNotNull()))\n",
    "\n",
    "    #Descomponemos la columna \"usernames\" en filas individuales\n",
    "    df = df_combined_mention.selectExpr(\"explode(username) as usernames\")\n",
    "\n",
    "    # Contamos el número de veces que aparece cada usuario\n",
    "    user_counts = df.groupBy(\"usernames\").count()\n",
    "\n",
    "    #Ordenamos en orden descendente por conteo y seleccionar los 10 primeros usuarios\n",
    "    top_10_users = user_counts.orderBy(col(\"count\").desc()).limit(10)\n",
    "\n",
    "    # Recopilamos los resultados en una lista de tuplas (usuario, conteo)\n",
    "    result_list = [(row.usernames, row[\"count\"]) for row in top_10_users.collect()]\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f8607759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 19:46:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "23/09/26 19:46:37 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 ms, sys: 0 ns, total: 20.7 ms\n",
      "Wall time: 3.54 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 19:46:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 ms, sys: 0 ns, total: 12.3 ms\n",
      "Wall time: 1.99 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 581:===============================>                        (9 + 7) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 ms, sys: 0 ns, total: 10.3 ms\n",
      "Wall time: 1.88 s\n",
      "\n",
      "\n",
      "[('2021-02-19', 'Preetm91'), ('2021-02-18', 'neetuanjle_nitu'), ('2021-02-13', 'MaanDee08215437'), ('2021-02-17', 'RaaJVinderkaur'), ('2021-02-16', 'jot__b'), ('2021-02-23', 'preetysaini321'), ('2021-02-15', 'jot__b'), ('2021-02-14', 'Gurpreetd86'), ('2021-02-20', 'MangalJ23056160'), ('2021-02-12', 'rebelpacifist')]\n",
      "\n",
      "\n",
      "[Row(emoji='🙏', count=7286), Row(emoji='😂', count=3072), Row(emoji='🚜', count=2972), Row(emoji='✊', count=2411), Row(emoji='🌾', count=2363), Row(emoji='🏻', count=2080), Row(emoji='❤', count=1779), Row(emoji='🤣', count=1668), Row(emoji='🏽', count=1218), Row(emoji='👇', count=1108)]\n",
      "\n",
      "\n",
      "[('narendramodi', 2623), ('Kisanektamorcha', 2045), ('RakeshTikaitBKU', 1848), ('PMOIndia', 1560), ('GretaThunberg', 1274), ('RahulGandhi', 1252), ('rihanna', 1142), ('DelhiPolice', 1134), ('RaviSinghKA', 1127), ('UNHumanRights', 1057)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Modificar file_path:\n",
    "file_path = \"/home/david/Escritorio/farmers-protest-tweets-2021-2-4.json\"\n",
    "%time top_tweets_time = q1_time(file_path)\n",
    "%time top_emojis_time = q2_time(file_path)\n",
    "%time top_mentions_time = q3_time(file_path)\n",
    "\n",
    "print('\\n')\n",
    "print(top_tweets_time)\n",
    "print('\\n')\n",
    "print(top_emojis_time)\n",
    "print('\\n')\n",
    "print(top_mentions_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b20177",
   "metadata": {},
   "source": [
    "###      ENFOQUE OPTIMIZACIÓN DE MEMORIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134439ea",
   "metadata": {},
   "source": [
    "#### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "593e0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    #Creamos la sesión de Spark y cargamos el archivo\n",
    "    spark = SparkSession.builder.appName(\"TopDateUsers\").getOrCreate()\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    #Convertimos la columna de fecha (para que tome solo lo necesario)\n",
    "    df = df.withColumn(\"date\", date_format(col(\"date\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "    # Unimos los tweets originales y los tweets en quotedTweet\n",
    "    df_combined = df.select(\"date\", \"user.username\").union(df.select(\"quotedTweet.date\", \"quotedTweet.user.username\").filter(col(\"quotedTweet.date\").isNotNull() & col(\"quotedTweet.user.username\").isNotNull()))\n",
    "\n",
    "    #df con las 10 fechas con más tweets\n",
    "    #esta parte se podría parametrizar, para este caso es válido ya que sabemos a priori el tamaño a mostrar en la lista,\n",
    "    #pero de ser un tamaño n no sería conveniente.\n",
    "    \n",
    "    #Para liberar memoria realizo primero la agregación (para contar la cantidad por fecha) y luego obtengo el top ordenandolos\n",
    "    #En dos pasos separados\n",
    "    agg_df = df_combined.groupBy(\"date\").agg(count(\"*\").alias(\"count\"))\n",
    "    \n",
    "    # Ordenar los resultados\n",
    "    top_dates = agg_df.orderBy(desc(\"count\")).limit(10)\n",
    "        \n",
    "    #Obtenemos las 10 fechas con más tweets directamente como DataFrame\n",
    "    top_dates_df = top_dates.select(\"date\")\n",
    "\n",
    "    # Obtenemos las 10 fechas como lista\n",
    "    top_dates_list = [row.date for row in top_dates.collect()]\n",
    "\n",
    "    # Filtrar el DataFrame original\n",
    "    filtered_df = df_combined.filter(col(\"date\").isin(top_dates_list))\n",
    "\n",
    "    #Agrupamos el DataFrame por fecha y usuario y calcula el recuento de cada grupo\n",
    "    grouped_df = filtered_df.groupBy(\"date\", \"username\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "    # Usamos una ventana para obtener el usuario que más se repite por fecha mediante rank\n",
    "    window_spec = Window.partitionBy(\"date\").orderBy(desc(\"count\"))\n",
    "    top_users_df = grouped_df.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1)\n",
    "\n",
    "    # Seleccionamos las columnas necesarias\n",
    "    result_df = top_users_df.select(\"date\", \"username\", \"count\")\n",
    "    \n",
    "    #Ordenamos el DataFrame por count en orden descendente\n",
    "    result_df = result_df.orderBy(desc(\"count\"))\n",
    "\n",
    "    #Recopilamos los resultados en una lista de tuplas\n",
    "    result = result_df.collect()\n",
    "\n",
    "    #Conviertimos el resultado en una lista de tuplas\n",
    "    result_list = [(row.date, row.username) for row in result]\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd0714c",
   "metadata": {},
   "source": [
    "#### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b53fe0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Creamos la sesión de Spark y cargar el archivo\n",
    "    spark = SparkSession.builder.appName(\"TopEmojis\").getOrCreate()\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    # Función para dividir los emojis en caracteres individuales\n",
    "    def extract_individual_emojis(text):\n",
    "        return [c for c in text if c in emoji.EMOJI_DATA]\n",
    "\n",
    "    # UDF para dividir los emojis y entregar un arreglo\n",
    "    extract_individual_emojis_udf = udf(extract_individual_emojis, ArrayType(StringType()))\n",
    "\n",
    "    # Agregamos una columna \"emojis\" al DataFrame con emojis individuales (en un arreglo)\n",
    "    df = df.withColumn(\"emojis\", extract_individual_emojis_udf(df[\"content\"]))\n",
    "    \n",
    "    # Desagrupamos los arreglos del campo emojis en filas separadas (emoji)\n",
    "    df = df.select(\"emojis\").withColumn(\"emoji\", explode(\"emojis\")).filter(col(\"emoji\") != \"\")\n",
    "\n",
    "    # Contamos la frecuencia de cada emoji en todo el DataFrame\n",
    "    emoji_counts = df.groupBy(\"emoji\").count()\n",
    "\n",
    "    #Obtenemos los 10 emojis con mayor conteo (usando limit antes de ordenar)\n",
    "    top_10_emojis = emoji_counts.orderBy(desc(\"count\")).limit(10).collect()\n",
    "\n",
    "    # Liberamos la memoria eliminando el DataFrame df\n",
    "    df.unpersist()\n",
    "\n",
    "    return top_10_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0fd72",
   "metadata": {},
   "source": [
    "#### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ccc240ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Creamos una sesión de Spark\n",
    "    spark = SparkSession.builder.appName(\"TopMentionedUsers\").getOrCreate()\n",
    "\n",
    "    # Cargamos el archivo JSON en un DataFrame\n",
    "    df_mention = spark.read.json(file_path)\n",
    "\n",
    "    # Convertimos la columna date en el formato necesario\n",
    "    df_mention = df_mention.withColumn(\"date\", date_format(col(\"date\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "    # Unir los tweets originales y los tweets en quotedTweet\n",
    "    df_combined_mention = df_mention.select(explode(\"mentionedUsers.username\").alias(\"username\"))\n",
    "\n",
    "    # Contar el número de veces que aparece cada usuario\n",
    "    user_counts = df_combined_mention.groupBy(\"username\").count()\n",
    "\n",
    "    # Ordenar en orden descendente por conteo y seleccionar los 10 primeros usuarios\n",
    "    top_10_users = user_counts.orderBy(desc(\"count\")).limit(10)\n",
    "\n",
    "    # Recopilar los resultados en una lista de tuplas (usuario, conteo)\n",
    "    result_list = [(row.username, row[\"count\"]) for row in top_10_users.collect()]\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79e2b6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 19:46:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "23/09/26 19:46:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 ms, sys: 3.77 ms, total: 19.4 ms\n",
      "Wall time: 2.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/26 19:46:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.03 ms, sys: 3.28 ms, total: 11.3 ms\n",
      "Wall time: 1.69 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.85 ms, sys: 0 ns, total: 8.85 ms\n",
      "Wall time: 1.4 s\n",
      "\n",
      "\n",
      "[('2021-02-19', 'Preetm91'), ('2021-02-18', 'neetuanjle_nitu'), ('2021-02-13', 'MaanDee08215437'), ('2021-02-17', 'RaaJVinderkaur'), ('2021-02-16', 'jot__b'), ('2021-02-23', 'preetysaini321'), ('2021-02-15', 'jot__b'), ('2021-02-14', 'Gurpreetd86'), ('2021-02-20', 'MangalJ23056160'), ('2021-02-12', 'rebelpacifist')]\n",
      "\n",
      "\n",
      "[Row(emoji='🙏', count=7286), Row(emoji='😂', count=3072), Row(emoji='🚜', count=2972), Row(emoji='✊', count=2411), Row(emoji='🌾', count=2363), Row(emoji='🏻', count=2080), Row(emoji='❤', count=1779), Row(emoji='🤣', count=1668), Row(emoji='🏽', count=1218), Row(emoji='👇', count=1108)]\n",
      "\n",
      "\n",
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "# Modificar file_path:\n",
    "file_path = \"/home/david/Escritorio/farmers-protest-tweets-2021-2-4.json\"\n",
    "%time top_tweets_memory = q1_memory(file_path)\n",
    "%time top_mentions_memory = q2_memory(file_path)\n",
    "%time top_emojis_memory = q3_memory(file_path)\n",
    "\n",
    "print('\\n')\n",
    "print(top_tweets_memory)\n",
    "print('\\n')\n",
    "print(top_mentions_memory)\n",
    "print('\\n')\n",
    "print(top_emojis_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b9556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
